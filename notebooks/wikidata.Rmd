---
title: "Wikidata exploration"
output: html_notebook
---

```{r packages}
library(WikidataR)
library(tidyverse)
```

```{r loaddata}
cutoff <- as_date("2019-01-25")
df <- read_csv("./data/tagged_text_entities.csv")
eval <- read_csv("./data/scraped_stories_sections.csv") %>% mutate(X1="a") %>% clean_data(.)
```

We first need to check out the PERSON entities spacy captured along a couple dimensions. First, how much junk (i.e., non-people) are we capturing here? We don't want to exhaustively evaluate this, since we're going to re-run the spacy model on the expanded data. Second, journalists switch to last name only on second and beyond references (e.g., "Elizabeth Warren" to "Warren"), but that will make matching difficult. If we remove one-word person references, does that disproporionately impact any part of the data?

```{r entitycheck}
df %>% 
  filter(entity_type=="PERSON") %>% 
  select(doc_id, entity) %>% 
  distinct() %>% 
  left_join(eval %>% select(link, pub_date), by=c("doc_id"="link")) %>% 
  group_by(doc_id) %>% 
  summarize(entities=n(), multi_entities=sum(grepl(" ", entity)), pub_date=first(pub_date)) %>% 
  mutate(prepost=ifelse(pub_date>=cutoff, "post", "pre"), pct_lost=(entities-multi_entities)/entities) %>% 
  filter(pub_date>cutoff-15 & pub_date<cutoff+15) %>% 
  ggplot(aes(pct_lost, fill=prepost)) + 
  geom_histogram()


people <- df %>% 
  filter(entity_type=="PERSON" & grepl(" ", entity)) %>% 
  select(doc_id, entity) %>% 
  distinct() %>% 
  left_join(eval %>% select(link, pub_date), by=c("doc_id"="link")) %>% 
  filter(pub_date>cutoff-15 & pub_date<cutoff+15)
```

TODO: Evaluate the actual person entities within the data



Next, we can try searching for some of these entities within Wikidata, to abstract names to larger categories (e.g., politicians, businesspeople)

Wikidata doesn't seem like it's going to be a workable solution here - the mapping from people to abstract categories is less rigid than I would have hoped, so getting concrete characterizations would be a huge lift.

```{r wikidata}
test <- find_item("Elizabeth Warren")
test[1]
people$entity

```

We can still count the number of names per story before and after layoffs, and look for something interesting
```{r namecount}
people %>% 
  group_by(doc_id) %>% 
  summarize(entities=n(), pub_date=first(pub_date)) %>% 
  group_by(pub_date) %>% 
  summarize(peopleper=median(entities)) %>% 
  ggplot(aes(pub_date, peopleper)) + 
  geom_line() + 
  geom_vline(xintercept=cutoff)
  
```

The number of people typically mentioned doesn't exhibit a clear trend, but maybe worth digging into more with more data/normalizing by total token count.


