{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import requests\n",
    "import mediacloud.api\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data specifications\n",
    "\n",
    "The BuzzFeed layoffs were [announced](https://techcrunch.com/2019/01/23/buzzfeed-layoffs-2019/) on 2019/01/23. They [started](https://slate.com/technology/2019/01/buzzfeeds-layoffs-wont-kill-it-but-they-have-changed-it.html) on 2019/01/25 and continued into the beginning of the subsequent week, although many did not focus on the BuzzFeed News division.\n",
    "\n",
    "Several considerations:\n",
    "\n",
    "* The staggered nature of the layoffs could affect our ability to get a clean before/after break.\n",
    "* We'll have to parse out which layoffs occurred on the news teams.\n",
    "* Since the layoffs straddled a weekend, we'll want to normalize output to account for fluctuations in publishing volume by day of week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date range\n",
    "Let's start by getting two weeks before and after the layoffs were announced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(date, padding, service=None):\n",
    "    \"\"\"Get range of dates a certain time period around a date\"\"\"\n",
    "    date_encoded = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    start = date_encoded - datetime.timedelta(days = padding)\n",
    "    end = date_encoded + datetime.timedelta(days = padding)\n",
    "    \n",
    "    date_range = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days+1)]\n",
    "    date_range_encoded = [10000*dt_time.year + 100*dt_time.month + dt_time.day for dt_time in date_range]\n",
    "    \n",
    "    if service=='GDELT':\n",
    "        return date_range_encoded\n",
    "    elif service=='MC':\n",
    "        return (start, end)\n",
    "    elif service=='NEWS':\n",
    "        return (start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d'))\n",
    "    elif service=='ARCHIVE':\n",
    "        return (date_range_encoded[0], date_range_encoded[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from GDELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDELT_URL = 'http://data.gdeltproject.org/events/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = daterange('2019-01-23', 14, 'GDELT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris = ['{}.export.CSV.zip'.format(i) for i in dates]\n",
    "urls = ['{0}{1}'.format(GDELT_URL, i) for i in uris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [01:38<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "df_all = []\n",
    "\n",
    "for i in tqdm(urls):\n",
    "    page = urlopen(i)\n",
    "    zipfile = ZipFile(BytesIO(page.read()))\n",
    "    filename = zipfile.namelist()[0]\n",
    "    df = pd.read_csv(zipfile.open(filename), sep='\\t', header=None)\n",
    "    bf_links = df[df[57].str.contains('www.buzzfeed')][57]\n",
    "    df_filtered = pd.DataFrame({'urls': bf_links, 'date': filename})\n",
    "    df_all.append(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_all).to_csv('../data/GDELT_29days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from Media Cloud\n",
    "\n",
    "[API documentation](https://github.com/berkmancenter/mediacloud/blob/master/doc/api_2_0_spec/api_2_0_spec.md#grab-all-stories-in-the-new-york-times-during-october-2012)\n",
    "\n",
    "Media Cloud doesn't seem to actually track BuzzFeed News stories :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_mc = os.getenv(\"API_KEY_MC\")\n",
    "mc = mediacloud.api.MediaCloud(api_key_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = daterange('2019-01-23', 14, 'MC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = mc.storyList('media_id:6218')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from News API\n",
    "[Python client documentation](https://newsapi.org/docs/client-libraries/python)\n",
    "\n",
    "Time period too far back for the free plan :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_news = os.getenv(\"API_KEY_NEWS\")\n",
    "newsapi = NewsApiClient(api_key=api_key_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = daterange('2019-01-23', 14, 'NEWS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "NewsAPIException",
     "evalue": "{'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2019-12-09, but you have requested 2019-01-09. To extend this please upgrade to a paid plan.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNewsAPIException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-65601eb99e27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m all_articles = newsapi.get_everything(domains='buzzfeednews.com',\n\u001b[0;32m      2\u001b[0m                                       \u001b[0mfrom_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                       to=dates[1])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\buzzfeed_layoffs\\lib\\site-packages\\newsapi\\newsapi_client.py\u001b[0m in \u001b[0;36mget_everything\u001b[1;34m(self, q, qintitle, sources, domains, exclude_domains, from_param, to, language, sort_by, page, page_size)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# Check Status of Request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mNewsAPIException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewsAPIException\u001b[0m: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2019-12-09, but you have requested 2019-01-09. To extend this please upgrade to a paid plan.'}"
     ]
    }
   ],
   "source": [
    "all_articles = newsapi.get_everything(domains='buzzfeednews.com',\n",
    "                                      from_param=dates[0],\n",
    "                                      to=dates[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from the Internet Archive\n",
    "\n",
    "[CDX server API](https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server#basic-usage)\n",
    "\n",
    "[Paginated API](https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server#pagination-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = daterange('2019-01-23', 14, 'ARCHIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://web.archive.org/cdx/search/cdx?url=buzzfeednews.com&matchType=host&from={0}&to={0}&output=json'.format(dates[0], dates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get(URL).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in results[1:]:\n",
    "    urls.append(i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_urls = [i for i in urls if 'article' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_article_urls = list(set([i.split('?')[0] if '?' in i else i for i in article_urls \n",
    "     if '/js/' not in i and 'x0.25' not in i and '/track/' not in i and '/v2.9' not in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'urls': filtered_article_urls}).to_csv('../data/ARCHIVE_29days.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buzzfeed_layoffs",
   "language": "python",
   "name": "buzzfeed_layoffs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
