{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import mediacloud.api\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data specifications\n",
    "\n",
    "The BuzzFeed layoffs were [announced](https://techcrunch.com/2019/01/23/buzzfeed-layoffs-2019/) on 2019/01/23. They [started](https://slate.com/technology/2019/01/buzzfeeds-layoffs-wont-kill-it-but-they-have-changed-it.html) on 2019/01/25 and continued into the beginning of the subsequent week, although many did not focus on the BuzzFeed News division.\n",
    "\n",
    "Several considerations:\n",
    "\n",
    "* The staggered nature of the layoffs could affect our ability to get a clean before/after break.\n",
    "* We'll have to parse out which layoffs occurred on the news teams.\n",
    "* Since the layoffs straddled a weekend, we'll want to normalize output to account for fluctuations in publishing volume by day of week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date range\n",
    "Let's start by getting two weeks before and after the layoffs were announced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(date, padding, service=None):\n",
    "    \"\"\"Get range of dates a certain time period around a date\"\"\"\n",
    "    date_encoded = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    start = date_encoded - datetime.timedelta(days = padding)\n",
    "    end = date_encoded + datetime.timedelta(days = padding)\n",
    "    \n",
    "    date_range = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days+1)]\n",
    "    date_range_encoded = [10000*dt_time.year + 100*dt_time.month + dt_time.day for dt_time in date_range]\n",
    "    \n",
    "    if service=='GDELT':\n",
    "        return date_range_encoded\n",
    "    elif service=='MC':\n",
    "        return (start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from GDELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDELT_URL = 'http://data.gdeltproject.org/events/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = daterange('2019-01-23', 14, 'GDELT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris = ['{}.export.CSV.zip'.format(i) for i in dates]\n",
    "urls = ['{0}{1}'.format(GDELT_URL, i) for i in uris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [01:38<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "df_all = []\n",
    "\n",
    "for i in tqdm(urls):\n",
    "    page = urlopen(i)\n",
    "    zipfile = ZipFile(BytesIO(page.read()))\n",
    "    filename = zipfile.namelist()[0]\n",
    "    df = pd.read_csv(zipfile.open(filename), sep='\\t', header=None)\n",
    "    bf_links = df[df[57].str.contains('www.buzzfeed')][57]\n",
    "    df_filtered = pd.DataFrame({'urls': bf_links, 'date': filename})\n",
    "    df_all.append(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_all).to_csv('../data/GDELT_29days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from Media Cloud\n",
    "\n",
    "[API documentation](https://github.com/berkmancenter/mediacloud/blob/master/doc/api_2_0_spec/api_2_0_spec.md#grab-all-stories-in-the-new-york-times-during-october-2012)\n",
    "\n",
    "Media Cloud doesn't seem to actually track BuzzFeed News stories :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"API_KEY\")\n",
    "mc = mediacloud.api.MediaCloud(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = daterange('2019-01-23', 14, 'MC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = mc.storyList('media_id:6218')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buzzfeed_layoffs",
   "language": "python",
   "name": "buzzfeed_layoffs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
