---
title: "Text pipeline"
author: "Nick Hagar"
date: "1/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidytext)
library(cleanNLP)
library(hunspell)
library(mallet)

cnlp_init_spacy(model_name="en_core_web_lg")
```

```{r}
df <- read_csv('../data/scraped_stories_sections.csv') %>% distinct()

```

## Text cleaning
```{r}
text_clean <- function(s) {
  clean_string <- s %>% 
    #Get rid of javascript parameter junk
    gsub("\\{.*?\\}\n", "", .) %>% 
    #Get rid of extra whitespace/newlines
    gsub("\\s+"," ", .) %>% 
    #Get rid of embedded tweets
    gsub("\\}.*? Retweet Favorite", "", .)  %>%
    #Get rid of related stories
    gsub("Â·.*", "", .) %>% 
    #Get rid of Instagram embeds
    gsub(" View this photo on Instagram ", "", .) %>% 
    #Get rid of Instagram embeds
    gsub("Instagram: @[A-z]* ", "", .) %>% 
    #Get rid of Twitter attributions
    gsub("@[A-z]* // Twitter ", "", .)
  return(clean_string)
}

df <- df %>% mutate(text_body=text_clean(text_body))
```

## Length - spacy
```{r}
df_tokens <- read_csv('../data/tagged_text_tokens.csv')

df_tokens %>% filter(upos!="X" & upos!="SYM" & upos!="SPACE") %>% 
  mutate(token_len=str_length(token)) %>% 
  group_by(doc_id) %>% 
  summarize(tokens=n(), characters=sum(token_len, na.rm=T))

df_tokens %>% filter(doc_id=="https://www.buzzfeednews.com/article/claudiarosenbaum/kesha-drops-sex-assault-case-against-dr-luke")
```


## Length
```{r}
df <- df %>% 
  mutate(len_chars = str_length(text_body),
         len_words = str_count(text_body, "\\w+"))
```

## Sentiment scoring
```{r}
sentiment <- get_sentiments("bing")

df_sent <- df %>% 
  unnest_tokens(word, text_body) %>% 
  inner_join(sentiment) %>% 
  group_by(link, sentiment) %>% 
  summarize(words=n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from=sentiment, values_from=words) %>% 
  mutate_all(~replace(., is.na(.), 0))
```

## Text tagging
```{r}
tagged_text <- df %>% 
  rename("doc_id"=link, "text"=text_body) %>% 
  select(doc_id, text) %>% 
  cnlp_annotate()

tagged_text$token %>% write_csv('../data/tagged_text_tokens.csv')
tagged_text$entity %>% write_csv('../data/tagged_text_entities.csv')
```

## Tag filtering
To remove: PUNCT, SPACE, PROPN, SYM, NUM, X
```{r}
bad_tokens <- c('PUNCT', 'SPACE', 'PROPN', 'SYM', 'NUM', 'X')

filtered_tokens <- tagged_text$token %>% 
  filter(!upos %in% bad_tokens)
```

## Misspellings
```{r}
is_error <- function(token) {
  spell <- hunspell(token)
  error <- length(spell[[1]])
  return(error)
}

tokens_spelling <- filtered_tokens %>% 
  mutate(misspelled = is_error(token))

tokens_spelling %>% 
  filter(misspelled>0)
```

## Entities
```{r}
df_ent <- read_csv('../data/tagged_text_entities.csv')

df_ent %>% 
  filter(entity_type=='PERSON') %>% 
  group_by(doc_id) %>% 
  summarize(people=n())
```

## Sourcing - quotes and tweets
```{r}
df_tokens <- read_csv('../data/tagged_text_tokens.csv')

df_tokens %>% 
  filter(lemma=='"' | grepl('^@', token)) %>% 
  mutate(type=ifelse(grepl('^@', token), 'handle', 'quote')) %>% 
  group_by(doc_id, type) %>% 
  summarize(count=n()) %>% 
  mutate(count_halved=ifelse(type=="quote", count/2, count))
```

## Topics - mallet (stopword list)
```{r}
fileConn <- file("stopword.txt")  

file.create(empty_file <- tempfile())

docs <- mallet.import(df$link, df$text_body, empty_file)

mallet_model <- MalletLDA(9)
mallet_model$loadDocuments(docs)

mallet.word.freqs(mallet_model) %>% 
  arrange(desc(doc.freq)) %>% 
  mutate(words=as.character(words)) %>% 
  top_frac(.01, doc.freq) %>% 
  pull(words) %>% 
  writeLines(., con=fileConn)

```

## Topics - mallet (model training)
```{r}
data("stop_words")

df_nostop <- df %>% 
  unnest_tokens(word, text_body) %>% 
  anti_join(stop_words, by="word") %>% 
  group_by(link) %>% 
    summarize(text_body = paste(word, collapse = " "))

docs <- mallet.import(df_nostop$link, df_nostop$text_body, "stopword.txt")

mallet_model <- MalletLDA(8)
mallet_model$loadDocuments(docs)

mallet_model$setAlphaOptimization(20, 50)

mallet_model$train(1000)
```

```{r}
topic.words <- mallet.topic.words(mallet_model, smoothed=T, normalized=T)

mallet.top.words(mallet_model, topic.words[9,])

mallet.topic.labels(mallet_model, topic.words, 20)

mallet.doc.topics(mallet_model, normalized=T)

topic.words[8,]
```

