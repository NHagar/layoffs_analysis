---
title: "Text pipeline"
author: "Nick Hagar"
date: "1/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidytext)
library(cleanNLP)
library(hunspell)

cnlp_init_spacy(model_name="en_core_web_lg")
```

```{r}
df <- read_csv('../data/scraped_stories_sections.csv') %>% distinct()

```

## Text cleaning
```{r}
text_clean <- function(s) {
  clean_string <- s %>% 
    gsub("\\{.*?\\}", "", .) %>% 
    gsub("\\s+"," ", .) %>% 
    gsub('".*?\\}', "", .) %>% 
    gsub('^ ,.*?\\}', "", .)
  
  return(clean_string)
}

df <- df %>% mutate(text_body=text_clean(text_body))
```

## Length
```{r}
df <- df %>% 
  mutate(len_chars = str_length(text_body),
         len_words = str_count(text_body, "\\w+"))
```

## Sentiment scoring
```{r}
sentiment <- get_sentiments("bing")

df_sent <- df %>% 
  unnest_tokens(word, text_body) %>% 
  inner_join(sentiment) %>% 
  group_by(link, sentiment) %>% 
  summarize(words=n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from=sentiment, values_from=words) %>% 
  mutate_all(~replace(., is.na(.), 0))
```

## Text tagging
```{r}
tagged_text <- df %>% 
  rename("doc_id"=link, "text"=text_body) %>% 
  select(doc_id, text) %>% 
  cnlp_annotate()

tagged_text$token %>% write_csv('../data/tagged_text_tokens.csv')
tagged_text$entity %>% write_csv('../data/tagged_text_entities.csv')
```

## Tag filtering
To remove: PUNCT, SPACE, PROPN, SYM, NUM, X
```{r}
bad_tokens <- c('PUNCT', 'SPACE', 'PROPN', 'SYM', 'NUM', 'X')

filtered_tokens <- tagged_text$token %>% 
  filter(!upos %in% bad_tokens)
```

## Misspellings
```{r}
is_error <- function(token) {
  spell <- hunspell(token)
  error <- length(spell[[1]])
  return(error)
}

tokens_spelling <- filtered_tokens %>% 
  mutate(misspelled = is_error(token))

tokens_spelling %>% 
  filter(misspelled>0)
```

